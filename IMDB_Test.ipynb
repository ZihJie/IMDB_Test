{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解壓縮 tar.gz 檔案\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\") as tf:\n",
    "    tf.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertModel, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(name,site):\n",
    "    path = '/home/zihjie/Test/IMDB_bert_test/aclImdb/'+name+'/'+site\n",
    "    datalist = []\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.splitext(i)[1] == '.txt':     #选取后缀为txt的文件加入datalist\n",
    "            datalist.append(i)\n",
    "    \n",
    "    print('datalist successful')\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "    temporarylist = []\n",
    "    for txt in datalist:\n",
    "        data_path = os.path.join(path,txt)\n",
    "        \n",
    "        with open(data_path, encoding='utf-8') as file:\n",
    "            words = file.read().strip()\n",
    "            score = re.findall(r\"_(.+?).t\",txt)\n",
    "            temporarylist.append([words,score])\n",
    "    \n",
    "    print('temporarylist successful')\n",
    "    print('-'*50)\n",
    "    \n",
    "    \n",
    "    temporary_df = pd.DataFrame(temporarylist, columns=[\"text\", \"score\"])\n",
    "    print(\"Shape:\", temporary_df.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('df successful')\n",
    "    print('-'*50)\n",
    "\n",
    "    print(temporary_df.head(5))\n",
    "    print('-'*50)\n",
    "\n",
    "    \n",
    "    \n",
    "    return temporary_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalist successful\n",
      "--------------------------------------------------\n",
      "temporarylist successful\n",
      "--------------------------------------------------\n",
      "Shape: (12500, 2)\n",
      "df successful\n",
      "--------------------------------------------------\n",
      "                                                text score\n",
      "0  It's nice to see a film with real people with ...  [10]\n",
      "1  I saw this film at the Santa Barbara Film Fest...   [9]\n",
      "2  As anyone old enough knows, South Africa long ...  [10]\n",
      "3  If the movies are to be believed, Chinese ghos...   [7]\n",
      "4  This is a small film , few characters ,theatri...   [9]\n",
      "--------------------------------------------------\n",
      "datalist successful\n",
      "--------------------------------------------------\n",
      "temporarylist successful\n",
      "--------------------------------------------------\n",
      "Shape: (12500, 2)\n",
      "df successful\n",
      "--------------------------------------------------\n",
      "                                                text score\n",
      "0  Scary Movie 2 was a grave disappointment. Simp...   [2]\n",
      "1  This movie was boring! <br /><br />Yes, there ...   [2]\n",
      "2  I felt like I was watching the Fast and the Fu...   [3]\n",
      "3  My husband and I went to see this movie, being...   [2]\n",
      "4  May the saints preserve us, because this movie...   [2]\n",
      "--------------------------------------------------\n",
      "datalist successful\n",
      "--------------------------------------------------\n",
      "temporarylist successful\n",
      "--------------------------------------------------\n",
      "Shape: (12500, 2)\n",
      "df successful\n",
      "--------------------------------------------------\n",
      "                                                text score\n",
      "0  A Walt Disney MICKEY MOUSE Cartoon.<br /><br /...  [10]\n",
      "1  Working the night shift in a seedy police stat...   [8]\n",
      "2  I came across this movie on DVD purely by chan...   [9]\n",
      "3  Got the chance to see this at a friend's house...  [10]\n",
      "4  \"Haggard: The Movie\" is well written, well dir...  [10]\n",
      "--------------------------------------------------\n",
      "datalist successful\n",
      "--------------------------------------------------\n",
      "temporarylist successful\n",
      "--------------------------------------------------\n",
      "Shape: (12500, 2)\n",
      "df successful\n",
      "--------------------------------------------------\n",
      "                                                text score\n",
      "0  Imagine this...<br /><br />Whenever two people...   [1]\n",
      "1  The final frames of the original \"American Gra...   [4]\n",
      "2  The original Body and Soul (1947) is a masterp...   [1]\n",
      "3  Not really worth a review, but I suppose it's ...   [2]\n",
      "4  Forgettable pilot that never really explains w...   [2]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_pos_df = create_df('train','pos')\n",
    "train_neg_df = create_df('train','neg')\n",
    "test_pos_df = create_df('test','pos')\n",
    "test_neg_df = create_df('test','neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_df['label'] = pd.Series( np.ones( len(train_pos_df)))\n",
    "train_neg_df['label'] = pd.Series( np.zeros( len(train_neg_df)))\n",
    "test_pos_df['label'] = pd.Series( np.ones( len(test_pos_df)))\n",
    "test_neg_df['label'] = pd.Series( np.zeros( len(test_neg_df)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concate pos and neg\n",
    "train_df = train_pos_df.append(train_neg_df)\n",
    "test_df = test_pos_df.append(test_neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改dataframe裡的值'[' 跟 ']'\n",
    "train_df['score'] = train_df['score'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])\n",
    "train_df['text'] = train_df['text'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])\n",
    "\n",
    "test_df['score'] = test_df['score'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])\n",
    "test_df['text'] = test_df['text'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['score'] = train_df['score'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])\n",
    "test_df['score'] = test_df['score'].map(lambda x: str(x)[:-1]).map(lambda x: str(x)[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text score  label\n",
      "0  It's nice to see a film with real people with ...    10    1.0\n",
      "1  I saw this film at the Santa Barbara Film Fest...     9    1.0\n",
      "2  As anyone old enough knows, South Africa long ...    10    1.0\n",
      "3  If the movies are to be believed, Chinese ghos...     7    1.0\n",
      "4  This is a small film , few characters ,theatri...     9    1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the train and test dataframe\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   text    25000 non-null  object \n",
      " 1   score   25000 non-null  object \n",
      " 2   label   25000 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['score'] = train_df['score'].astype(int) \n",
    "test_df['score'] = test_df['score'].astype(int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].astype(str) \n",
    "test_df['text'] = test_df['text'].astype(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].astype(int) \n",
    "test_df['label'] = test_df['label'].astype(int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How, in the name of all that's holy, did this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I read reviews on this movie and decided to gi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And that is the only reason I posses this DVD....</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>**SPOILERS**KHAMOSH is totally unrealistic, la...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I saw the trailer for this film a few months p...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  label\n",
       "0  How, in the name of all that's holy, did this ...      1      0\n",
       "1  I read reviews on this movie and decided to gi...      1      0\n",
       "2  And that is the only reason I posses this DVD....      2      0\n",
       "3  **SPOILERS**KHAMOSH is totally unrealistic, la...      2      0\n",
       "4  I saw the trailer for this film a few months p...      4      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['It', 'is', 'slow', 'and', 'even', ',', 'sweet', 'and', 'moving',\n",
       "       '.', 'One', 'of', 'the', 'best', 'unless', 'you', 'like', 'car',\n",
       "       'chase', '##s', ',', 'sex', 'scenes', ',', 'and', 'violence', '.'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"It is slow and even, sweet and moving. One of the best \\\n",
    "unless you like car chases, sex scenes, and violence.\"\n",
    "tokens = tokenizer.tokenize(text)      # 每個字切詞成一個list\n",
    "print(type(tokens))                 # list\n",
    "np.array(tokens)                    # 轉成numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1135, 1110, 3345, 1105, 1256,  117, 4105, 1105, 2232,  119, 1448,\n",
       "       1104, 1103, 1436, 4895, 1128, 1176, 1610, 9839, 1116,  117, 2673,\n",
       "       4429,  117, 1105, 4289,  119])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)   # 每個字轉成id\n",
    "print(type(input_ids))                         # list\n",
    "print(len(input_ids))\n",
    "np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids = tokenizer.create_token_type_ids_from_sequences(input_ids) \n",
    "# token_type_ids 必須input還沒加CLS SEP\n",
    "print(type(token_type_ids))                                # list\n",
    "print(len(token_type_ids)) \n",
    "np.array(token_type_ids)\n",
    "# 多兩個token是CLS、SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id add CLS and SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 101, 1135, 1110, 3345, 1105, 1256,  117, 4105, 1105, 2232,  119,\n",
       "       1448, 1104, 1103, 1436, 4895, 1128, 1176, 1610, 9839, 1116,  117,\n",
       "       2673, 4429,  117, 1105, 4289,  119,  102])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "print(type(input_ids))\n",
    "print(len(input_ids))\n",
    "np.array(input_ids)\n",
    "# 101是CLS，101是SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fix input_id's dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 101, 1135, 1110, 3345, 1105, 1256,  117, 4105, 1105, 2232,  119,\n",
       "       1448, 1104, 1103, 1436, 4895, 1128, 1176, 1610, 9839, 1116,  117,\n",
       "       2673, 4429,  117, 1105, 4289,  119,  102,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 512 - len(input_ids)\n",
    "input_ids2 = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0))  \n",
    "# array右邊append n 個 0  補長度到512\n",
    "print(len(input_ids2))\n",
    "input_ids2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把上述處理建立一個function，丟入一串文字試試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定在512維\n",
    "def input_ids_all(text):\n",
    "    if len(text)>510:\n",
    "        text = text[0:510]        \n",
    "    tokens = tokenizer.tokenize(text)                          # 每個字切詞成一個list\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)        # 每個字轉成id\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "    input_ids = np.array(input_ids)                            # list 轉 numpy\n",
    "    if len(input_ids) < 512:\n",
    "        n = 512 - len(input_ids)\n",
    "        input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0))    \n",
    "        # array右邊append n 個 0  補長度到512\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定再64維\n",
    "def input_ids_all_64(text):\n",
    "    if len(text)>62:\n",
    "        text = text[0:62]        \n",
    "    tokens = tokenizer.tokenize(text)                          # 每個字切詞成一個list\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)        # 每個字轉成id\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "    input_ids = np.array(input_ids)                            # list 轉 numpy\n",
    "    if len(input_ids) < 64:\n",
    "        n = 64 - len(input_ids)\n",
    "        input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0))    \n",
    "        # array右邊append n 個 0  補長度到512\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定再128維\n",
    "def input_ids_all_128(text):\n",
    "    if len(text)>126:\n",
    "        text = text[0:126]        \n",
    "    tokens = tokenizer.tokenize(text)                          # 每個字切詞成一個list\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)        # 每個字轉成id\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "    input_ids = np.array(input_ids)                            # list 轉 numpy\n",
    "    if len(input_ids) < 128:\n",
    "        n = 128 - len(input_ids)\n",
    "        input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0))    \n",
    "        # array右邊append n 個 0  補長度到512\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定再128維(改)\n",
    "def input_ids_all_128(text):\n",
    "    tokens = tokenizer.tokenize(text)                          # 每個字切詞成一個list\n",
    "    if len(tokens)>126:\n",
    "        tokens = tokens[0:126] \n",
    "        \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)        # 每個字轉成id\n",
    "   \n",
    "    \n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "    input_ids = np.array(input_ids)                            # list 轉 numpy\n",
    "    \n",
    "    if len(input_ids) < 128:\n",
    "        n = 128 - len(input_ids)\n",
    "        input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0))    \n",
    "        # array右邊append n 個 0  補長度到512\n",
    "    return input_ids\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention mask tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mask_all(text):\n",
    "    tokens = tokenizer.tokenize(text)       # 每個字切詞成一個list\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)  # 每個字轉成id\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)    # 句子前後加上 CLS SEP 的 id\n",
    "    input_ids = np.array(input_ids)          # list 轉 numpy\n",
    "    attention_mask = np.array([1,1])\n",
    "    attention_mask = np.pad(attention_mask, (0, len(input_ids)-2), mode ='constant', constant_values=(1)) \n",
    "    #array右邊append 1 到跟segment一樣長\n",
    "    if len(attention_mask) < 512:\n",
    "        n = 512 - len(attention_mask)\n",
    "        attention_mask = np.pad(attention_mask, (0, n), mode ='constant', constant_values=(0))  \n",
    "        # array右邊append n 個 0  補長度到512\n",
    "    return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_all(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"How, in the name of all that's holy, did this film ever get distribution? It looks as if it has been shot on someone's mobile phone and takes the screaming girl victim scenario to whole new depths. They literally scream for the full 90 minutes of the movie. And that's all they do. There is no plot, no tension, no characters, and not a lot of acting. Just screaming and more screaming.<br /><br />I gave up after fifteen minutes and fast-wound through it to see if anything happened. It doesn't - except for screaming, of course. Odlly enough, the act of going through it on fast forward highlights another problem - there is no camera-work to speak of. Every shot looks like every other shot - middle distance, one angle, dull, dull, DULL.<br /><br />It's not so bad it's good. It's just plain bad.\",\n",
       "       'I read reviews on this movie and decided to give it a shot. I\\'m an open minded guy after all and I’ve given good reviews to some pretty bad flicks. As the end credits rolled on this one I searched for meaning and something nice to say. Here goes: \"This film was mercifully short.\" That\\'s all I got.<br /><br />Okay, Okay. The sets and visuals were well done and the music helped lend to the mood of asylum life but the film was painful to watch and the endless dialogue took away from the good bits. I did find myself laughing at this film but the way you laugh at your best friend who just embarrassed himself in front of a large crowd.<br /><br />By the time of the \"chicken dance\" at the finale I had just decided to tuck and roll with the film and let the bodies fall where they fall. I don\\'t know what could have salvaged this film. The acting was not bad and it looked like it had a budget but there just wasn\\'t any way to make it watchable; not even the presence of beautiful bare breasts. Maybe I should have sparked a doobie or drank a LOT of beer to get the full experience of the film. Either way, I\\'m not watching this film again unless I\\'m really depressed. Then I can tell myself “At least I wasn’t in ‘Dr. Tarr\\'s Torture Dungeon.’ I’m better than those guys.\"',\n",
       "       'And that is the only reason I posses this DVD. Now I haven\\'t seen the first Nemesis film, but I did check the info out of it and I here by say: What? Why? Because in the first film Alex was male. But then again the first one was set in the future, so maybe this Alex is brand new one and the scientist just happened to make Alex female this time. Who knows, at least it wasn\\'t addressed in the film in any way.<br /><br />Here\\'s a quick summary of the plot: Alex, still a baby then (or how ever you want, as it was, is, in the future) escapes with her mom using a special time vessel and ends up in the 80\\'s Africa. There mommy gets killed and Alex (Sue Price) grows up in a African tribe. Then the tribe gets slaughtered by a cyborg from the future and Alex then runs and hides and finally she kills the cyborg. So there. Does sound familiar, doesn\\'t it?.<br /><br />Terminator isn\\'t the only film being ripped here, Predator gets its fair share too and I think the first Fly movie, the Vincent Price one, gets special nomination for giving a solid base to build up your cyborgs head from.<br /><br />Lets see, what else? Okay, the film was quite standard small budget flick, but it did have bad special effects for a mid 90\\'s film. It would have looked okay for a 80\\'s flick how ever. Biggest problem is the plot. Things just happen and the viewer is barely interested. Nemesis 2 isn\\'t the crappiest piece of cinema I\\'ve had pleasure (?) to watch but it does come damn close.<br /><br />I won\\'t say a thing about acting, because let\\'s be honest here: did anyone expect Oscar worthy performances here? Oh well... at least I did find Sue Price hot in that amazonian warrior way.<br /><br />A \"real\" movie rating: 2/10 There isn\\'t a lot of pros about the over all quality. And despite of the very basic plot the film it self makes very little sense.<br /><br />A camp movie rating: 4/10 I did get occasional laughs from the sheer badness of the film, so it does have small merits in it.',\n",
       "       ...,\n",
       "       'After reading the original play I thought it would have been much more difficult to adapt to screen than it turned out to be. Donal McCann puts in a once-off great performance as Public Gar, the repressed antagonist who is manifested openly on screen by his extroverted (but unseen to others) alterego- Private Gar. Eamonn Kelly also plays an excellent \"screwballs\" whose inability to communicate his feelings is matched only by Gar.<br /><br />Definitely worth renting out if you can find it. (Probably unavailable outside Ireland & UK)',\n",
       "       'This is a very dramatic and suspenseful movie. There are many plots and turns. The story or the director opens question marks on the death row or presumed crimes committed by black people. This film is very well directed by Arne Glimcher and the fine sound of James Newton Howard is excellent. Strong performance of Sean Connery and Ed Harris. If you liked this one don´t miss \"TRUE CRIMES\" or \"THE HURRICANE\". My wife and me gave 8/10.',\n",
       "       \"Let me give a quick summery of the film: A rotten, rude kid named Max stumbles upon a radio that contains Kazaam: a rapping genie. Like all genies, he grants 3 wishes but, being good natured, also helps Max with his personal life, as he has to deal with bullies and a father mixed up in organized crime. During all this, Kazaam raps from time to time, (also showcasing Shaq's dismal rap skills).<br /><br />This movie proves what we all know: Athletes need to stick to sports. I admit that it never looked like an Oscar-worthy movie, but EVERYTHING about this waste of film is horrible. The characters are either unlikable or stupid, the plot is not even worth mentioning, the dialog is a joke, and Shaq is only a quarter of the problem. Hell, even if Denzel Washington played Kazaam this movie would still be a joke. I know that the movie only drew ANYBODY was because Shaq was so big (no pun intended) at the time. I honestly cannot think of a single positive thing to say about this waste of time. Shaq should have put the time had used to make this movie toward practicing free throws.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_df['text'].to_numpy()\n",
    "print(len(text))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1731,  117, ..., 1104, 1736,  102],\n",
       "       [ 101,  146, 2373, ...,  119,  146,  102],\n",
       "       [ 101, 1262, 1115, ...,  170, 2963,  102],\n",
       "       ...,\n",
       "       [ 101, 1258, 3455, ..., 2270,  111,  102],\n",
       "       [ 101, 1188, 1110, ...,    0,    0,    0],\n",
       "       [ 101, 2421, 1143, ...,  131, 1335,  102]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = [input_ids_all_128(i) for i in text]      # 必須要 [ ] 輸出是list\n",
    "input_ids = np.array(input_ids)                    # 轉成numpy\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   146,  2373,  3761,  1113,  1142,  2523,  1105,  1879,\n",
       "        1106,  1660,  1122,   170,  2046,   119,   146,   112,   182,\n",
       "        1126,  1501, 13767,  2564,  1170,  1155,  1105,   146,   787,\n",
       "        1396,  1549,  1363,  3761,  1106,  1199,  2785,  2213, 22302,\n",
       "        1116,   119,  1249,  1103,  1322,  6459,  3733,  1113,  1142,\n",
       "        1141,   146,  8703,  1111,  2764,  1105,  1380,  3505,  1106,\n",
       "        1474,   119,  3446,  2947,   131,   107,  1188,  1273,  1108,\n",
       "        1143, 19878, 17126,  1193,  1603,   119,   107,  1337,   112,\n",
       "         188,  1155,   146,  1400,   119,   133,  9304,   120,   135,\n",
       "         133,  9304,   120,   135,  3956,   117,  3956,   119,  1109,\n",
       "        3741,  1105,  5173,  1116,  1127,  1218,  1694,  1105,  1103,\n",
       "        1390,  2375, 23559,  1106,  1103,  6601,  1104, 15345,  1297,\n",
       "        1133,  1103,  1273,  1108,  8920,  1106,  2824,  1105,  1103,\n",
       "       12401,  8556,  1261,  1283,  1121,  1103,  1363,  9182,   119,\n",
       "         146,   102])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = train_df['label'].to_numpy()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "# # 設定 stratify = label 把每個類別平均\n",
    "train_input_ids, validation_input_ids, train_label, validation_label = train_test_split(input_ids, label, \n",
    "                                                            random_state=2018, test_size=0.2, stratify=label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
    "# num_labels=5 分5類\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "5000/5000 [==============================] - 300s 60ms/step - loss: 0.4578 - accuracy: 0.8005 - val_loss: 0.4121 - val_accuracy: 0.7918\n",
      "Epoch 2/4\n",
      "5000/5000 [==============================] - 300s 60ms/step - loss: 0.3793 - accuracy: 0.8612 - val_loss: 0.4294 - val_accuracy: 0.8312\n",
      "Epoch 3/4\n",
      "5000/5000 [==============================] - 300s 60ms/step - loss: 0.3889 - accuracy: 0.8661 - val_loss: 0.4624 - val_accuracy: 0.8256\n",
      "Epoch 4/4\n",
      "5000/5000 [==============================] - 301s 60ms/step - loss: 0.3382 - accuracy: 0.9042 - val_loss: 0.5128 - val_accuracy: 0.8274\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()  # batch size 8就會error 可能是記憶體爆掉\n",
    "model_fit = model.fit(train_input_ids, train_label, \n",
    "                      batch_size=4, epochs=4, \n",
    "                      validation_data=(validation_input_ids, validation_label)\n",
    "#                    ,steps_per_epoch=115\n",
    "#                    validation_steps=7)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
