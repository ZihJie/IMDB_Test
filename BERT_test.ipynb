{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\" #英文pretrain(不區分大小寫)\n",
    "print(torch.__version__)\n",
    "#1.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入BERT token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 30522\n",
      "token               index          \n",
      "-------------------------\n",
      "basin                6403\n",
      "leads                5260\n",
      "[unused937]           942\n",
      "commonwealth         5663\n",
      "intimacy            20893\n",
      "oldest               4587\n",
      "##xley              20959\n",
      "slant               27474\n",
      "killers             15978\n",
      "several              2195\n"
     ]
    }
   ],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n",
    "\n",
    "# see some token and index mapping\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids): #隨便看幾個字\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,random_split\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def preprocess html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('_text(sen):\n",
    "    # Removing[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zihjie/Test/IMDB_bert_test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIMDB(path, seg):\n",
    "    classes = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf-8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([preprocess_text(review), 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([preprocess_text(review), 0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改\n",
    "def readIMDB(path, seg):\n",
    "    classes = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            try:\n",
    "                rf =  open(os.path.join(path, seg, label, file), 'r', encoding='utf-8') \n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([preprocess_text(review), 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([preprocess_text(review), 0])\n",
    "            except:\n",
    "                print('maybe some error')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 'neg', 1: 'pos'}\n",
    "\n",
    "#create Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.df = readIMDB('/home/zihjie/Test/IMDB_bert_test/aclImdb',mode) #its list [['text1',label],['text2',label],...]\n",
    "        self.len = len(self.df)\n",
    "        self.maxlen = 300 #限制文章長度(若你記憶體夠多也可以不限)\n",
    "        self.tokenizer = tokenizer  # we will use BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        origin_text = self.df[idx][0]\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            #label_tensor = None #in our case, we have label\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:     \n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a[:self.maxlen] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        if text_b is not None:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            word_pieces += tokens_b + [\"[SEP]\"]\n",
    "            len_b = len(word_pieces) - len_a\n",
    "               \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        if text_b is None:\n",
    "            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)\n",
    "        elif text_b is not None:\n",
    "            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, origin_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maybe some error\n",
      "maybe some error\n",
      "maybe some error\n",
      "maybe some error\n",
      "trainset size: 24000\n",
      "valset size: 1000\n",
      "testset size:  25000\n"
     ]
    }
   ],
   "source": [
    "# initialize Dataset\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "testset = MyDataset(\"test\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#split val from trainset\n",
    "val_size = int(trainset.__len__()*0.04) #比對LSTM 切出1000筆當validation\n",
    "trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1045,  2061,  2293,  2023,  3185,  1996,  7284,  2003,  2307,\n",
       "          2005, 13433,  2243, 12256,  3185,  1996,  1039,  5856,  3504,  2061,\n",
       "         12476,  2293,  1996,  2189,  1999,  1996,  3185,  2061,  2307,  2027,\n",
       "          2921,  1996,  2887,  2189,  2004,  2005,  1996,  2466,  2049,  2307,\n",
       "          2009,  2038,  2307,  3110,  1997,  6860,  8292,  2571,  5638,  2003,\n",
       "          2200, 10140,  1998,  3928, 13433,  2243, 12256,  6683,  2003,  2428,\n",
       "          2307,  1999,  2023,  3185,  1998,  2066,  2010,  6860,  2007,  3520,\n",
       "          1996,  2069,  2518,  2134,  2066,  2001, 24086, 10841,  2638,  3311,\n",
       "          2002,  2074,  3402, 16949,  2039,  7126,  6683,  2522,  2978,  1998,\n",
       "          3727,  2027,  2071,  2031,  2081,  2010,  2112,  1999,  1996,  3185,\n",
       "          2210,  7046,  2021,  3452, 12476,  3185,  2064,  3524,  2000,  2219,\n",
       "          1996,  3915,  2544,  2006,  4966,   102]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor(1),\n",
       " 'I so love this movie The animation is great for pok mon movie the cgi looks so awesome love the music in the movie So great they kept the Japanese music As for the story its great It has great feeling of friendship Celebi is very cute and powerful pok mon Ash is really great in this movie and like his friendship with Sam The only thing didn like was Suicune appearance he just suddenly pops up helps Ash co bit and leaves They could have made his part in the movie little bigger But overall awesome movie Can wait to own the USA version on dvd ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:\n",
      " ['[CLS]', 'well', 'when', 'before', 'saw', 'this', 'film', 'really', 'wasn', 'sure', 'whether', 'it', 'would', 'be', 'my', 'cup', 'of', 'tea', 'how', 'wrong', 'was', 'thought', 'that', 'this', 'was', 'one', 'of', 'the', 'best', 'films', 've', 'watched', 'for', 'very', 'long', 'time', 'real', 'family', 'classic', 'the', 'story', 'of', 'young', 'eva', '##cu', '##ee', 'and', 'his', 'new', 'foster', 'dad', 'this', 'film', 'tick', '##s', 'all', 'the', 'boxes', 've', 'not', 'read', 'the', 'book', 'maybe', 'that', 'a', 'good', 'thing', 'meant', 'enjoyed', 'the', 'film', 'more', 'but', 'with', 'regards', 'to', 'the', 'story', 'really', 'can', 'think', 'of', 'any', 'bad', 'points', 'hence', 'scoring', 'it', 'out', 'of', 'and', 'hardly', 'ever', 'think', 'anything', 'warrant', '##s', 'top', 'marks', 'by', 'the', 'time', 'william', 'proclaimed', 'can', 'ride', 'my', 'bike', 'dad', 'was', 'sobbing', 'my', 'heart', 'out', 'anyone', 'who', 'seen', 'it', 'will', 'understand', 'm', 'sure', 'really', 'heart', '##war', '##ming', 'and', 'definitely', 'recommended', '[SEP]'] \n",
      "\n",
      "origin_text:\n",
      " Well when before saw this film really wasn sure whether it would be my cup of tea how wrong was thought that this was one of the best films ve watched for very long time real family classic The story of young evacuee and his new foster dad this film ticks all the boxes ve not read the book maybe that a good thing meant enjoyed the film more but with regards to the story really can think of any bad points hence scoring it out of and hardly ever think anything warrants top marks By the time William proclaimed CAN RIDE MY BIKE DAD was sobbing my heart out anyone who seen it will understand m sure Really heartwarming and definitely recommended  \n",
      "\n",
      "label: pos \n",
      "\n",
      "tokens_tensor:\n",
      " tensor([  101,  2092,  2043,  2077,  2387,  2023,  2143,  2428,  2347,  2469,\n",
      "         3251,  2009,  2052,  2022,  2026,  2452,  1997,  5572,  2129,  3308,\n",
      "         2001,  2245,  2008,  2023,  2001,  2028,  1997,  1996,  2190,  3152,\n",
      "         2310,  3427,  2005,  2200,  2146,  2051,  2613,  2155,  4438,  1996,\n",
      "         2466,  1997,  2402,  9345, 10841,  4402,  1998,  2010,  2047,  6469,\n",
      "         3611,  2023,  2143, 16356,  2015,  2035,  1996,  8378,  2310,  2025,\n",
      "         3191,  1996,  2338,  2672,  2008,  1037,  2204,  2518,  3214,  5632,\n",
      "         1996,  2143,  2062,  2021,  2007, 12362,  2000,  1996,  2466,  2428,\n",
      "         2064,  2228,  1997,  2151,  2919,  2685,  6516,  4577,  2009,  2041,\n",
      "         1997,  1998,  6684,  2412,  2228,  2505, 10943,  2015,  2327,  6017,\n",
      "         2011,  1996,  2051,  2520, 10116,  2064,  4536,  2026,  7997,  3611,\n",
      "         2001, 20040,  2026,  2540,  2041,  3087,  2040,  2464,  2009,  2097,\n",
      "         3305,  1049,  2469,  2428,  2540,  9028,  6562,  1998,  5791,  6749,\n",
      "          102]) \n",
      "\n",
      "segment tensor:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 隨便選一個樣本\n",
    "sample_idx = 10\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,origin_text = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "\n",
    "print('token:\\n',tokens,'\\n')\n",
    "print('origin_text:\\n',origin_text,'\\n')\n",
    "print('label:',label_map[int(label_tensor.numpy())],'\\n')\n",
    "print('tokens_tensor:\\n',tokens_tensor,'\\n')\n",
    "print('segment tensor:\\n',segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2023,  3185,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  3585,  ...,     0,     0,     0],\n",
      "        [  101, 17081,  2003,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2026,  2434,  ...,  2000,  4109,   102],\n",
      "        [  101,  2023,  3185,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
    "#截長補短後要限制attention只注意非pad 的部分\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad到該batch下最長的長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader\n",
    "# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(tokens_tensors)\n",
    "print(segments_tensors)\n",
    "print(masks_tensors)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name      module\n",
      "--------------------\n",
      "bert      embeddings\n",
      "bert      encoder\n",
      "bert      pooler\n",
      "dropout    Dropout(p=0.1, inplace=False)\n",
      "classifier Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "name      module\n",
    "--------------------\"\"\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(\"{:10}{}\".format(name,n) )\n",
    "    else:\n",
    "        print(\"{:10} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "[epoch 1] loss: 0.2744, acc: 0.8830, val loss: 0.231500, val acc: 0.919643\n",
      "[epoch 2] loss: 0.1521, acc: 0.9447, val loss: 0.221421, val acc: 0.914683\n",
      "[epoch 3] loss: 0.0874, acc: 0.9704, val loss: 0.278775, val acc: 0.918651\n",
      "[epoch 4] loss: 0.0501, acc: 0.9843, val loss: 0.294110, val acc: 0.918651\n",
      "[epoch 5] loss: 0.0360, acc: 0.9890, val loss: 0.353868, val acc: 0.915675\n",
      "[epoch 6] loss: 0.0287, acc: 0.9916, val loss: 0.437387, val acc: 0.898810\n",
      "[epoch 7] loss: 0.0214, acc: 0.9936, val loss: 0.432030, val acc: 0.900794\n",
      "[epoch 8] loss: 0.0200, acc: 0.9938, val loss: 0.416535, val acc: 0.906746\n",
      "[epoch 9] loss: 0.0167, acc: 0.9949, val loss: 0.433508, val acc: 0.909722\n",
      "[epoch 10] loss: 0.0116, acc: 0.9966, val loss: 0.508724, val acc: 0.912698\n",
      "Done\n",
      "CPU times: user 1h 14min 38s, sys: 23min 46s, total: 1h 38min 24s\n",
      "Wall time: 1h 38min 25s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    train_loss , val_loss = 0.0 , 0.0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        n += 1\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        # outputs 的順序是 \"(loss), logits, (hidden_states), (attentions)\"\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get prediction and calulate acc\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            m += 1\n",
    "            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "            val_outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "            logits = val_outputs[1]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "            val_loss += val_outputs[0].item()\n",
    "\n",
    "    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %\n",
    "          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "        val_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors, \n",
    "                    labels=labels)\n",
    "\n",
    "        logits = val_outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "c = confusion_matrix(true, predictions)\n",
    "print(c)\n",
    "accuracy_score(predictions,true)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
